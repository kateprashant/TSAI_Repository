{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MultiClass_Ass_Try9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kateprashant/TSAI_Repository/blob/master/MultiClass_Ass_Try9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESj6yssmfB7r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "911c9a9a-d2b6-4ed5-83d8-a276b37fb093"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOG_3R3df-1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "\n",
        "import os\n",
        "import shutil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sosLJlVgG2_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "533285ae-3f33-4ff6-f62b-c718571b18d9"
      },
      "source": [
        "# load annotations\n",
        "\n",
        "drive_mount = os.path.abspath('./gdrive')\n",
        "base_dir = os.path.join(drive_mount, 'My Drive/assignment_5')\n",
        "model_dir = os.path.join(base_dir, 'saved_models')\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()\n",
        "df.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13573, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwC4tKq-gT3-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "outputId": "efb1f3f1-2818-4acd-c2a2-930a7daeafe0"
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T\n",
        "one_hot_df.describe().T\n",
        "display(one_hot_df)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>resized/5.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13568</th>\n",
              "      <td>resized/13570.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13569</th>\n",
              "      <td>resized/13571.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13570</th>\n",
              "      <td>resized/13572.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13571</th>\n",
              "      <td>resized/13573.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13572</th>\n",
              "      <td>resized/13574.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13573 rows × 28 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "0          resized/1.jpg              0  ...                        1              0\n",
              "1          resized/2.jpg              1  ...                        1              0\n",
              "2          resized/3.jpg              0  ...                        1              0\n",
              "3          resized/4.jpg              0  ...                        1              0\n",
              "4          resized/5.jpg              1  ...                        1              0\n",
              "...                  ...            ...  ...                      ...            ...\n",
              "13568  resized/13570.jpg              0  ...                        1              0\n",
              "13569  resized/13571.jpg              1  ...                        1              0\n",
              "13570  resized/13572.jpg              1  ...                        0              1\n",
              "13571  resized/13573.jpg              1  ...                        1              0\n",
              "13572  resized/13574.jpg              0  ...                        1              0\n",
              "\n",
              "[13573 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ0fiyN0ia4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MZd4LucnhTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    def __init__(self, df, batch_size=32, shuffle=True, aug_list=[], incl_orig=True):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.aug_list = aug_list\n",
        "        self.incl_orig = incl_orig\n",
        "        self.orig_len = int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __len__(self):\n",
        "      if self.incl_orig:\n",
        "            delta = 1\n",
        "      else:\n",
        "            delta = 0\n",
        "      return self.orig_len * (len(self.aug_list) + delta)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "       if not self.incl_orig :\n",
        "            index += self.orig_len - 1\n",
        "\n",
        "       if index > self.orig_len - 1:\n",
        "            aug = self.aug_list[index // self.orig_len - 1]\n",
        "            index %= self.orig_len\n",
        "       else:\n",
        "            aug = None\n",
        "\n",
        "       batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "       items = self.df.iloc[batch_slice]\n",
        "        \n",
        "       images = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])        \n",
        "        \n",
        "       if aug is not None:\n",
        "            images = aug.flow(images, shuffle=False).next()\n",
        "        \n",
        "       target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        \n",
        "       return images, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVljObz5nyxv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fa13e651-119d-4bee-d1f4-494864ddfd2a"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIRgjO7NooM7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0411c36c-791c-4b08-9f47-a0bf75ef5f6c"
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=32)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "print('no. of units:', num_units)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no. of units: {'gender': 2, 'image_quality': 3, 'age': 5, 'weight': 4, 'bag': 3, 'pose': 3, 'footwear': 3, 'emotion': 4}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_AljFVrfVcg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b1c42b0c-f9f3-43c8-96e2-9bdf58501b41"
      },
      "source": [
        "backbone = VGG16(\n",
        "    weights=None, \n",
        "    include_top=False, \n",
        "    input_tensor=Input(shape=(224, 224, 3))\n",
        ")\n",
        "\n",
        "neck = backbone.output\n",
        "neck = Flatten(name=\"flatten\")(neck)\n",
        "neck = Dense(512, activation=\"relu\")(neck)\n",
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "        neck = Dropout(0.3)(in_layer)\n",
        "        neck = Dense(128, activation=\"relu\")(neck)\n",
        "        return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "  # heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "\n",
        "model = Model(\n",
        "          inputs=backbone.input, \n",
        "          outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion],\n",
        "      )\n",
        "\n",
        "model.compile(\n",
        "          optimizer=SGD(lr=1e-3, momentum=0.9),\n",
        "          loss=\"categorical_crossentropy\", \n",
        "          metrics=[\"accuracy\"]\n",
        "          )   \n",
        "\n",
        "model.summary()\n",
        " "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 224, 224, 64) 1792        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 224, 224, 64) 36928       block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_pool (MaxPooling2D)      (None, 112, 112, 64) 0           block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv1 (Conv2D)           (None, 112, 112, 128 73856       block1_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv2 (Conv2D)           (None, 112, 112, 128 147584      block2_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 56, 56, 128)  0           block2_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv1 (Conv2D)           (None, 56, 56, 256)  295168      block2_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv2 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv3 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 28, 28, 256)  0           block3_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv1 (Conv2D)           (None, 28, 28, 512)  1180160     block3_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 14, 14, 512)  0           block4_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv1 (Conv2D)           (None, 14, 14, 512)  2359808     block4_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv2 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv3 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_pool (MaxPooling2D)      (None, 7, 7, 512)    0           block5_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 25088)        0           block5_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 512)          12845568    flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 128)          65664       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 128)          65664       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 128)          65664       dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 128)          65664       dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 128)          65664       dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 128)          65664       dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 128)          65664       dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 128)          65664       dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "gender_output (Dense)           (None, 2)            258         dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "image_quality_output (Dense)    (None, 3)            387         dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "age_output (Dense)              (None, 5)            645         dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "weight_output (Dense)           (None, 4)            516         dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bag_output (Dense)              (None, 3)            387         dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "footwear_output (Dense)         (None, 3)            387         dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "pose_output (Dense)             (None, 3)            387         dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "emotion_output (Dense)          (None, 4)            516         dense_8[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 28,089,051\n",
            "Trainable params: 28,089,051\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0KWOrdumdP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# freeze backbone\n",
        "for layer in backbone.layers:\n",
        "\tlayer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRccE-HZn5KQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e8e2914e-607f-4c77-9362-e314b61879ce"
      },
      "source": [
        "from keras.callbacks import LearningRateScheduler\n",
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=50,\n",
        "    verbose =2,\n",
        "     callbacks=[\n",
        "            ModelCheckpoint(\n",
        "                os.path.join(model_dir, 'model_{epoch:03d}.hdf5'),\n",
        "                save_best_only=True,\n",
        "                verbose=1,\n",
        "            ),\n",
        "        ],\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\n",
            " - 92s - loss: 7.7824 - gender_output_loss: 0.6825 - image_quality_output_loss: 0.9797 - age_output_loss: 1.4304 - weight_output_loss: 0.9836 - bag_output_loss: 0.9213 - footwear_output_loss: 0.9454 - pose_output_loss: 0.9299 - emotion_output_loss: 0.9097 - gender_output_acc: 0.5593 - image_quality_output_acc: 0.5532 - age_output_acc: 0.3983 - weight_output_acc: 0.6355 - bag_output_acc: 0.5622 - footwear_output_acc: 0.5658 - pose_output_acc: 0.6146 - emotion_output_acc: 0.7143 - val_loss: 7.6929 - val_gender_output_loss: 0.6665 - val_image_quality_output_loss: 0.9759 - val_age_output_loss: 1.4248 - val_weight_output_loss: 0.9875 - val_bag_output_loss: 0.8970 - val_footwear_output_loss: 0.9095 - val_pose_output_loss: 0.8950 - val_emotion_output_loss: 0.9368 - val_gender_output_acc: 0.5716 - val_image_quality_output_acc: 0.5529 - val_age_output_acc: 0.3926 - val_weight_output_acc: 0.6366 - val_bag_output_acc: 0.5706 - val_footwear_output_acc: 0.5958 - val_pose_output_acc: 0.6381 - val_emotion_output_acc: 0.6946\n",
            "\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 7.69289, saving model to /content/gdrive/My Drive/assignment_5/saved_models/model_001.hdf5\n",
            "Epoch 2/50\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 7.69289, saving model to /content/gdrive/My Drive/assignment_5/saved_models/model_001.hdf5\n",
            " - 91s - loss: 7.6488 - gender_output_loss: 0.6484 - image_quality_output_loss: 0.9784 - age_output_loss: 1.4139 - weight_output_loss: 0.9813 - bag_output_loss: 0.9072 - footwear_output_loss: 0.8931 - pose_output_loss: 0.9195 - emotion_output_loss: 0.9070 - gender_output_acc: 0.6077 - image_quality_output_acc: 0.5521 - age_output_acc: 0.3955 - weight_output_acc: 0.6349 - bag_output_acc: 0.5641 - footwear_output_acc: 0.5931 - pose_output_acc: 0.6143 - emotion_output_acc: 0.7143 - val_loss: 7.5784 - val_gender_output_loss: 0.6455 - val_image_quality_output_loss: 0.9675 - val_age_output_loss: 1.4177 - val_weight_output_loss: 0.9751 - val_bag_output_loss: 0.8841 - val_footwear_output_loss: 0.8701 - val_pose_output_loss: 0.8788 - val_emotion_output_loss: 0.9397 - val_gender_output_acc: 0.6376 - val_image_quality_output_acc: 0.5529 - val_age_output_acc: 0.3931 - val_weight_output_acc: 0.6366 - val_bag_output_acc: 0.5811 - val_footwear_output_acc: 0.6104 - val_pose_output_acc: 0.6381 - val_emotion_output_acc: 0.6946\n",
            "\n",
            "Epoch 00002: val_loss improved from 7.69289 to 7.57843, saving model to /content/gdrive/My Drive/assignment_5/saved_models/model_002.hdf5\n",
            "Epoch 3/50\n",
            " - 91s - loss: 7.4832 - gender_output_loss: 0.6099 - image_quality_output_loss: 0.9740 - age_output_loss: 1.4037 - weight_output_loss: 0.9708 - bag_output_loss: 0.8926 - footwear_output_loss: 0.8577 - pose_output_loss: 0.8734 - emotion_output_loss: 0.9010 - gender_output_acc: 0.6620 - image_quality_output_acc: 0.5521 - age_output_acc: 0.3982 - weight_output_acc: 0.6348 - bag_output_acc: 0.5750 - footwear_output_acc: 0.6115 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7146 - val_loss: 7.4167 - val_gender_output_loss: 0.5887 - val_image_quality_output_loss: 0.9686 - val_age_output_loss: 1.4112 - val_weight_output_loss: 0.9741 - val_bag_output_loss: 0.8742 - val_footwear_output_loss: 0.8568 - val_pose_output_loss: 0.8035 - val_emotion_output_loss: 0.9395 - val_gender_output_acc: 0.6976 - val_image_quality_output_acc: 0.5529 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6346 - val_bag_output_acc: 0.5902 - val_footwear_output_acc: 0.6159 - val_pose_output_acc: 0.6487 - val_emotion_output_acc: 0.6946\n",
            "\n",
            "Epoch 00003: val_loss improved from 7.57843 to 7.41666, saving model to /content/gdrive/My Drive/assignment_5/saved_models/model_003.hdf5\n",
            "Epoch 4/50\n",
            "\n",
            "Epoch 00003: val_loss improved from 7.57843 to 7.41666, saving model to /content/gdrive/My Drive/assignment_5/saved_models/model_003.hdf5\n",
            " - 91s - loss: 7.3292 - gender_output_loss: 0.5820 - image_quality_output_loss: 0.9699 - age_output_loss: 1.3954 - weight_output_loss: 0.9668 - bag_output_loss: 0.8819 - footwear_output_loss: 0.8368 - pose_output_loss: 0.8022 - emotion_output_loss: 0.8941 - gender_output_acc: 0.6911 - image_quality_output_acc: 0.5528 - age_output_acc: 0.3959 - weight_output_acc: 0.6350 - bag_output_acc: 0.5878 - footwear_output_acc: 0.6193 - pose_output_acc: 0.6495 - emotion_output_acc: 0.7145 - val_loss: 7.3144 - val_gender_output_loss: 0.5643 - val_image_quality_output_loss: 0.9754 - val_age_output_loss: 1.4097 - val_weight_output_loss: 0.9694 - val_bag_output_loss: 0.8702 - val_footwear_output_loss: 0.8544 - val_pose_output_loss: 0.7386 - val_emotion_output_loss: 0.9324 - val_gender_output_acc: 0.7092 - val_image_quality_output_acc: 0.5529 - val_age_output_acc: 0.3931 - val_weight_output_acc: 0.6376 - val_bag_output_acc: 0.5716 - val_footwear_output_acc: 0.6169 - val_pose_output_acc: 0.6815 - val_emotion_output_acc: 0.6946\n",
            "\n",
            "Epoch 00004: val_loss improved from 7.41666 to 7.31440, saving model to /content/gdrive/My Drive/assignment_5/saved_models/model_004.hdf5\n",
            "Epoch 5/50\n",
            " - 91s - loss: 7.1592 - gender_output_loss: 0.5516 - image_quality_output_loss: 0.9612 - age_output_loss: 1.3909 - weight_output_loss: 0.9638 - bag_output_loss: 0.8692 - footwear_output_loss: 0.8158 - pose_output_loss: 0.7220 - emotion_output_loss: 0.8849 - gender_output_acc: 0.7173 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4010 - weight_output_acc: 0.6348 - bag_output_acc: 0.5968 - footwear_output_acc: 0.6335 - pose_output_acc: 0.6970 - emotion_output_acc: 0.7144 - val_loss: 7.2280 - val_gender_output_loss: 0.5400 - val_image_quality_output_loss: 0.9584 - val_age_output_loss: 1.4081 - val_weight_output_loss: 0.9837 - val_bag_output_loss: 0.8792 - val_footwear_output_loss: 0.8036 - val_pose_output_loss: 0.7032 - val_emotion_output_loss: 0.9518 - val_gender_output_acc: 0.7359 - val_image_quality_output_acc: 0.5544 - val_age_output_acc: 0.3936 - val_weight_output_acc: 0.6341 - val_bag_output_acc: 0.5932 - val_footwear_output_acc: 0.6477 - val_pose_output_acc: 0.6951 - val_emotion_output_acc: 0.6946\n",
            "\n",
            "Epoch 00005: val_loss improved from 7.31440 to 7.22795, saving model to /content/gdrive/My Drive/assignment_5/saved_models/model_005.hdf5\n",
            "Epoch 6/50\n",
            " - 91s - loss: 7.0249 - gender_output_loss: 0.5175 - image_quality_output_loss: 0.9574 - age_output_loss: 1.3831 - weight_output_loss: 0.9577 - bag_output_loss: 0.8617 - footwear_output_loss: 0.8026 - pose_output_loss: 0.6694 - emotion_output_loss: 0.8755 - gender_output_acc: 0.7425 - image_quality_output_acc: 0.5553 - age_output_acc: 0.4014 - weight_output_acc: 0.6361 - bag_output_acc: 0.6055 - footwear_output_acc: 0.6448 - pose_output_acc: 0.7232 - emotion_output_acc: 0.7142 - val_loss: 7.1471 - val_gender_output_loss: 0.5195 - val_image_quality_output_loss: 0.9508 - val_age_output_loss: 1.3982 - val_weight_output_loss: 0.9631 - val_bag_output_loss: 0.8613 - val_footwear_output_loss: 0.7965 - val_pose_output_loss: 0.7305 - val_emotion_output_loss: 0.9272 - val_gender_output_acc: 0.7303 - val_image_quality_output_acc: 0.5585 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.6376 - val_bag_output_acc: 0.5892 - val_footwear_output_acc: 0.6512 - val_pose_output_acc: 0.7092 - val_emotion_output_acc: 0.6946\n",
            "\n",
            "Epoch 00006: val_loss improved from 7.22795 to 7.14711, saving model to /content/gdrive/My Drive/assignment_5/saved_models/model_006.hdf5\n",
            "Epoch 7/50\n",
            " - 91s - loss: 7.0249 - gender_output_loss: 0.5175 - image_quality_output_loss: 0.9574 - age_output_loss: 1.3831 - weight_output_loss: 0.9577 - bag_output_loss: 0.8617 - footwear_output_loss: 0.8026 - pose_output_loss: 0.6694 - emotion_output_loss: 0.8755 - gender_output_acc: 0.7425 - image_quality_output_acc: 0.5553 - age_output_acc: 0.4014 - weight_output_acc: 0.6361 - bag_output_acc: 0.6055 - footwear_output_acc: 0.6448 - pose_output_acc: 0.7232 - emotion_output_acc: 0.7142 - val_loss: 7.1471 - val_gender_output_loss: 0.5195 - val_image_quality_output_loss: 0.9508 - val_age_output_loss: 1.3982 - val_weight_output_loss: 0.9631 - val_bag_output_loss: 0.8613 - val_footwear_output_loss: 0.7965 - val_pose_output_loss: 0.7305 - val_emotion_output_loss: 0.9272 - val_gender_output_acc: 0.7303 - val_image_quality_output_acc: 0.5585 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.6376 - val_bag_output_acc: 0.5892 - val_footwear_output_acc: 0.6512 - val_pose_output_acc: 0.7092 - val_emotion_output_acc: 0.6946\n",
            "\n",
            "Epoch 00006: val_loss improved from 7.22795 to 7.14711, saving model to /content/gdrive/My Drive/assignment_5/saved_models/model_006.hdf5\n",
            " - 90s - loss: 6.8700 - gender_output_loss: 0.4729 - image_quality_output_loss: 0.9485 - age_output_loss: 1.3723 - weight_output_loss: 0.9508 - bag_output_loss: 0.8476 - footwear_output_loss: 0.7864 - pose_output_loss: 0.6215 - emotion_output_loss: 0.8699 - gender_output_acc: 0.7700 - image_quality_output_acc: 0.5565 - age_output_acc: 0.4040 - weight_output_acc: 0.6373 - bag_output_acc: 0.6214 - footwear_output_acc: 0.6503 - pose_output_acc: 0.7425 - emotion_output_acc: 0.7141 - val_loss: 6.9585 - val_gender_output_loss: 0.4641 - val_image_quality_output_loss: 0.9506 - val_age_output_loss: 1.3943 - val_weight_output_loss: 0.9622 - val_bag_output_loss: 0.8363 - val_footwear_output_loss: 0.8044 - val_pose_output_loss: 0.6104 - val_emotion_output_loss: 0.9362 - val_gender_output_acc: 0.7717 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3972 - val_weight_output_acc: 0.6316 - val_bag_output_acc: 0.6144 - val_footwear_output_acc: 0.6482 - val_pose_output_acc: 0.7490 - val_emotion_output_acc: 0.6946\n",
            "\n",
            "Epoch 00007: val_loss improved from 7.14711 to 6.95853, saving model to /content/gdrive/My Drive/assignment_5/saved_models/model_007.hdf5\n",
            "Epoch 8/50\n",
            " - 91s - loss: 6.7344 - gender_output_loss: 0.4421 - image_quality_output_loss: 0.9439 - age_output_loss: 1.3638 - weight_output_loss: 0.9440 - bag_output_loss: 0.8343 - footwear_output_loss: 0.7699 - pose_output_loss: 0.5742 - emotion_output_loss: 0.8623 - gender_output_acc: 0.7931 - image_quality_output_acc: 0.5582 - age_output_acc: 0.4062 - weight_output_acc: 0.6354 - bag_output_acc: 0.6270 - footwear_output_acc: 0.6623 - pose_output_acc: 0.7655 - emotion_output_acc: 0.7148 - val_loss: 6.8691 - val_gender_output_loss: 0.4409 - val_image_quality_output_loss: 0.9509 - val_age_output_loss: 1.3923 - val_weight_output_loss: 0.9647 - val_bag_output_loss: 0.8310 - val_footwear_output_loss: 0.7753 - val_pose_output_loss: 0.5986 - val_emotion_output_loss: 0.9154 - val_gender_output_acc: 0.7898 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3936 - val_weight_output_acc: 0.6361 - val_bag_output_acc: 0.6215 - val_footwear_output_acc: 0.6532 - val_pose_output_acc: 0.7520 - val_emotion_output_acc: 0.6946\n",
            "\n",
            "Epoch 00008: val_loss improved from 6.95853 to 6.86909, saving model to /content/gdrive/My Drive/assignment_5/saved_models/model_008.hdf5\n",
            "Epoch 9/50\n",
            " - 91s - loss: 6.6158 - gender_output_loss: 0.4130 - image_quality_output_loss: 0.9382 - age_output_loss: 1.3517 - weight_output_loss: 0.9339 - bag_output_loss: 0.8230 - footwear_output_loss: 0.7560 - pose_output_loss: 0.5450 - emotion_output_loss: 0.8550 - gender_output_acc: 0.8102 - image_quality_output_acc: 0.5593 - age_output_acc: 0.4148 - weight_output_acc: 0.6387 - bag_output_acc: 0.6369 - footwear_output_acc: 0.6682 - pose_output_acc: 0.7778 - emotion_output_acc: 0.7141 - val_loss: 6.8922 - val_gender_output_loss: 0.4306 - val_image_quality_output_loss: 0.9525 - val_age_output_loss: 1.3965 - val_weight_output_loss: 0.9700 - val_bag_output_loss: 0.8231 - val_footwear_output_loss: 0.7941 - val_pose_output_loss: 0.5980 - val_emotion_output_loss: 0.9273 - val_gender_output_acc: 0.8070 - val_image_quality_output_acc: 0.5685 - val_age_output_acc: 0.3957 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.6310 - val_footwear_output_acc: 0.6568 - val_pose_output_acc: 0.7686 - val_emotion_output_acc: 0.6946\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 6.86909\n",
            "Epoch 10/50\n",
            " - 91s - loss: 6.4825 - gender_output_loss: 0.3807 - image_quality_output_loss: 0.9271 - age_output_loss: 1.3425 - weight_output_loss: 0.9201 - bag_output_loss: 0.8121 - footwear_output_loss: 0.7441 - pose_output_loss: 0.5088 - emotion_output_loss: 0.8472 - gender_output_acc: 0.8322 - image_quality_output_acc: 0.5622 - age_output_acc: 0.4122 - weight_output_acc: 0.6407 - bag_output_acc: 0.6470 - footwear_output_acc: 0.6751 - pose_output_acc: 0.7964 - emotion_output_acc: 0.7145 - val_loss: 6.8826 - val_gender_output_loss: 0.4395 - val_image_quality_output_loss: 0.9301 - val_age_output_loss: 1.3936 - val_weight_output_loss: 0.9677 - val_bag_output_loss: 0.8392 - val_footwear_output_loss: 0.7771 - val_pose_output_loss: 0.6043 - val_emotion_output_loss: 0.9310 - val_gender_output_acc: 0.8090 - val_image_quality_output_acc: 0.5675 - val_age_output_acc: 0.4128 - val_weight_output_acc: 0.6336 - val_bag_output_acc: 0.6331 - val_footwear_output_acc: 0.6517 - val_pose_output_acc: 0.7560 - val_emotion_output_acc: 0.6941\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 6.86909\n",
            "Epoch 11/50\n",
            " - 91s - loss: 6.3308 - gender_output_loss: 0.3476 - image_quality_output_loss: 0.9215 - age_output_loss: 1.3244 - weight_output_loss: 0.9085 - bag_output_loss: 0.7935 - footwear_output_loss: 0.7243 - pose_output_loss: 0.4722 - emotion_output_loss: 0.8387 - gender_output_acc: 0.8491 - image_quality_output_acc: 0.5616 - age_output_acc: 0.4256 - weight_output_acc: 0.6464 - bag_output_acc: 0.6577 - footwear_output_acc: 0.6874 - pose_output_acc: 0.8109 - emotion_output_acc: 0.7148 - val_loss: 6.8479 - val_gender_output_loss: 0.4327 - val_image_quality_output_loss: 0.9387 - val_age_output_loss: 1.4047 - val_weight_output_loss: 0.9580 - val_bag_output_loss: 0.8141 - val_footwear_output_loss: 0.7815 - val_pose_output_loss: 0.5902 - val_emotion_output_loss: 0.9279 - val_gender_output_acc: 0.8034 - val_image_quality_output_acc: 0.5640 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.6326 - val_footwear_output_acc: 0.6527 - val_pose_output_acc: 0.7853 - val_emotion_output_acc: 0.6941\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 6.86909\n",
            "Epoch 11/50\n",
            "\n",
            "Epoch 00011: val_loss improved from 6.86909 to 6.84787, saving model to /content/gdrive/My Drive/assignment_5/saved_models/model_011.hdf5\n",
            "Epoch 12/50\n",
            " - 91s - loss: 6.1838 - gender_output_loss: 0.3224 - image_quality_output_loss: 0.9092 - age_output_loss: 1.3112 - weight_output_loss: 0.8911 - bag_output_loss: 0.7740 - footwear_output_loss: 0.7086 - pose_output_loss: 0.4398 - emotion_output_loss: 0.8274 - gender_output_acc: 0.8568 - image_quality_output_acc: 0.5690 - age_output_acc: 0.4273 - weight_output_acc: 0.6464 - bag_output_acc: 0.6696 - footwear_output_acc: 0.6882 - pose_output_acc: 0.8263 - emotion_output_acc: 0.7153 - val_loss: 6.8641 - val_gender_output_loss: 0.4248 - val_image_quality_output_loss: 0.9617 - val_age_output_loss: 1.3931 - val_weight_output_loss: 0.9566 - val_bag_output_loss: 0.8340 - val_footwear_output_loss: 0.7877 - val_pose_output_loss: 0.5809 - val_emotion_output_loss: 0.9252 - val_gender_output_acc: 0.8145 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.3921 - val_weight_output_acc: 0.6371 - val_bag_output_acc: 0.6250 - val_footwear_output_acc: 0.6613 - val_pose_output_acc: 0.7777 - val_emotion_output_acc: 0.6956\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 6.84787\n",
            "Epoch 13/50\n",
            " - 91s - loss: 6.0052 - gender_output_loss: 0.2879 - image_quality_output_loss: 0.8956 - age_output_loss: 1.2833 - weight_output_loss: 0.8788 - bag_output_loss: 0.7503 - footwear_output_loss: 0.6894 - pose_output_loss: 0.4017 - emotion_output_loss: 0.8183 - gender_output_acc: 0.8777 - image_quality_output_acc: 0.5791 - age_output_acc: 0.4432 - weight_output_acc: 0.6569 - bag_output_acc: 0.6836 - footwear_output_acc: 0.6970 - pose_output_acc: 0.8453 - emotion_output_acc: 0.7160 - val_loss: 6.8434 - val_gender_output_loss: 0.4139 - val_image_quality_output_loss: 0.9405 - val_age_output_loss: 1.3988 - val_weight_output_loss: 0.9563 - val_bag_output_loss: 0.8217 - val_footwear_output_loss: 0.7944 - val_pose_output_loss: 0.5895 - val_emotion_output_loss: 0.9284 - val_gender_output_acc: 0.8155 - val_image_quality_output_acc: 0.5620 - val_age_output_acc: 0.3800 - val_weight_output_acc: 0.6391 - val_bag_output_acc: 0.6346 - val_footwear_output_acc: 0.6628 - val_pose_output_acc: 0.7772 - val_emotion_output_acc: 0.6870\n",
            "\n",
            "Epoch 00013: val_loss improved from 6.84787 to 6.84337, saving model to /content/gdrive/My Drive/assignment_5/saved_models/model_013.hdf5\n",
            "Epoch 14/50\n",
            " - 91s - loss: 5.8084 - gender_output_loss: 0.2763 - image_quality_output_loss: 0.8780 - age_output_loss: 1.2512 - weight_output_loss: 0.8531 - bag_output_loss: 0.7251 - footwear_output_loss: 0.6676 - pose_output_loss: 0.3569 - emotion_output_loss: 0.8001 - gender_output_acc: 0.8845 - image_quality_output_acc: 0.5855 - age_output_acc: 0.4568 - weight_output_acc: 0.6589 - bag_output_acc: 0.6965 - footwear_output_acc: 0.7097 - pose_output_acc: 0.8628 - emotion_output_acc: 0.7183 - val_loss: 6.9417 - val_gender_output_loss: 0.4436 - val_image_quality_output_loss: 0.9481 - val_age_output_loss: 1.4101 - val_weight_output_loss: 0.9658 - val_bag_output_loss: 0.8216 - val_footwear_output_loss: 0.7972 - val_pose_output_loss: 0.6125 - val_emotion_output_loss: 0.9427 - val_gender_output_acc: 0.8004 - val_image_quality_output_acc: 0.5645 - val_age_output_acc: 0.3851 - val_weight_output_acc: 0.6321 - val_bag_output_acc: 0.6341 - val_footwear_output_acc: 0.6542 - val_pose_output_acc: 0.7747 - val_emotion_output_acc: 0.6935\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 6.84337\n",
            "Epoch 15/50\n",
            " - 90s - loss: 5.5691 - gender_output_loss: 0.2351 - image_quality_output_loss: 0.8634 - age_output_loss: 1.2220 - weight_output_loss: 0.8234 - bag_output_loss: 0.6865 - footwear_output_loss: 0.6391 - pose_output_loss: 0.3212 - emotion_output_loss: 0.7786 - gender_output_acc: 0.9038 - image_quality_output_acc: 0.5960 - age_output_acc: 0.4749 - weight_output_acc: 0.6698 - bag_output_acc: 0.7102 - footwear_output_acc: 0.7223 - pose_output_acc: 0.8764 - emotion_output_acc: 0.7213 - val_loss: 7.1548 - val_gender_output_loss: 0.4596 - val_image_quality_output_loss: 0.9582 - val_age_output_loss: 1.4311 - val_weight_output_loss: 1.0125 - val_bag_output_loss: 0.8364 - val_footwear_output_loss: 0.8142 - val_pose_output_loss: 0.6478 - val_emotion_output_loss: 0.9950 - val_gender_output_acc: 0.8150 - val_image_quality_output_acc: 0.5544 - val_age_output_acc: 0.3906 - val_weight_output_acc: 0.6154 - val_bag_output_acc: 0.6200 - val_footwear_output_acc: 0.6593 - val_pose_output_acc: 0.7692 - val_emotion_output_acc: 0.6941\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 6.84337\n",
            "Epoch 00014: val_loss did not improve from 6.84337\n",
            "Epoch 15/50\n",
            "Epoch 16/50\n",
            "\n",
            "Epoch 16/50\n",
            " - 91s - loss: 5.2967 - gender_output_loss: 0.2155 - image_quality_output_loss: 0.8327 - age_output_loss: 1.1698 - weight_output_loss: 0.7821 - bag_output_loss: 0.6512 - footwear_output_loss: 0.6055 - pose_output_loss: 0.2925 - emotion_output_loss: 0.7474 - gender_output_acc: 0.9114 - image_quality_output_acc: 0.6139 - age_output_acc: 0.5018 - weight_output_acc: 0.6852 - bag_output_acc: 0.7305 - footwear_output_acc: 0.7319 - pose_output_acc: 0.8865 - emotion_output_acc: 0.7288 - val_loss: 7.4257 - val_gender_output_loss: 0.5196 - val_image_quality_output_loss: 0.9756 - val_age_output_loss: 1.4977 - val_weight_output_loss: 1.0592 - val_bag_output_loss: 0.8682 - val_footwear_output_loss: 0.8590 - val_pose_output_loss: 0.6596 - val_emotion_output_loss: 0.9868 - val_gender_output_acc: 0.8080 - val_image_quality_output_acc: 0.5544 - val_age_output_acc: 0.3649 - val_weight_output_acc: 0.5761 - val_bag_output_acc: 0.6406 - val_footwear_output_acc: 0.6502 - val_pose_output_acc: 0.7853 - val_emotion_output_acc: 0.6830\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 6.84337\n",
            "Epoch 17/50\n",
            " - 91s - loss: 5.2967 - gender_output_loss: 0.2155 - image_quality_output_loss: 0.8327 - age_output_loss: 1.1698 - weight_output_loss: 0.7821 - bag_output_loss: 0.6512 - footwear_output_loss: 0.6055 - pose_output_loss: 0.2925 - emotion_output_loss: 0.7474 - gender_output_acc: 0.9114 - image_quality_output_acc: 0.6139 - age_output_acc: 0.5018 - weight_output_acc: 0.6852 - bag_output_acc: 0.7305 - footwear_output_acc: 0.7319 - pose_output_acc: 0.8865 - emotion_output_acc: 0.7288 - val_loss: 7.4257 - val_gender_output_loss: 0.5196 - val_image_quality_output_loss: 0.9756 - val_age_output_loss: 1.4977 - val_weight_output_loss: 1.0592 - val_bag_output_loss: 0.8682 - val_footwear_output_loss: 0.8590 - val_pose_output_loss: 0.6596 - val_emotion_output_loss: 0.9868 - val_gender_output_acc: 0.8080 - val_image_quality_output_acc: 0.5544 - val_age_output_acc: 0.3649 - val_weight_output_acc: 0.5761 - val_bag_output_acc: 0.6406 - val_footwear_output_acc: 0.6502 - val_pose_output_acc: 0.7853 - val_emotion_output_acc: 0.6830\n",
            " - 91s - loss: 5.0034 - gender_output_loss: 0.1958 - image_quality_output_loss: 0.8013 - age_output_loss: 1.1128 - weight_output_loss: 0.7375 - bag_output_loss: 0.6068 - footwear_output_loss: 0.5730 - pose_output_loss: 0.2607 - emotion_output_loss: 0.7155 - gender_output_acc: 0.9209 - image_quality_output_acc: 0.6320 - age_output_acc: 0.5284 - weight_output_acc: 0.7019 - bag_output_acc: 0.7474 - footwear_output_acc: 0.7514 - pose_output_acc: 0.8989 - emotion_output_acc: 0.7400 - val_loss: 7.4968 - val_gender_output_loss: 0.5185 - val_image_quality_output_loss: 0.9665 - val_age_output_loss: 1.4942 - val_weight_output_loss: 1.0414 - val_bag_output_loss: 0.8877 - val_footwear_output_loss: 0.8789 - val_pose_output_loss: 0.7211 - val_emotion_output_loss: 0.9884 - val_gender_output_acc: 0.8085 - val_image_quality_output_acc: 0.5499 - val_age_output_acc: 0.3674 - val_weight_output_acc: 0.6129 - val_bag_output_acc: 0.6255 - val_footwear_output_acc: 0.6336 - val_pose_output_acc: 0.7792 - val_emotion_output_acc: 0.6875\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 6.84337\n",
            "Epoch 18/50\n",
            "\n",
            " - 91s - loss: 4.6483 - gender_output_loss: 0.1786 - image_quality_output_loss: 0.7527 - age_output_loss: 1.0409 - weight_output_loss: 0.6800 - bag_output_loss: 0.5599 - footwear_output_loss: 0.5437 - pose_output_loss: 0.2260 - emotion_output_loss: 0.6666 - gender_output_acc: 0.9278 - image_quality_output_acc: 0.6560 - age_output_acc: 0.5598 - weight_output_acc: 0.7253 - bag_output_acc: 0.7677 - footwear_output_acc: 0.7660 - pose_output_acc: 0.9151 - emotion_output_acc: 0.7507 - val_loss: 8.2818 - val_gender_output_loss: 0.5470 - val_image_quality_output_loss: 1.0974 - val_age_output_loss: 1.6353 - val_weight_output_loss: 1.1536 - val_bag_output_loss: 0.9922 - val_footwear_output_loss: 0.9437 - val_pose_output_loss: 0.8147 - val_emotion_output_loss: 1.0979 - val_gender_output_acc: 0.8120 - val_image_quality_output_acc: 0.5131 - val_age_output_acc: 0.3498 - val_weight_output_acc: 0.5721 - val_bag_output_acc: 0.6341 - val_footwear_output_acc: 0.6331 - val_pose_output_acc: 0.7762 - val_emotion_output_acc: 0.6678\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 6.84337\n",
            "Epoch 19/50\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 6.84337\n",
            "Epoch 19/50\n",
            " - 91s - loss: 4.2622 - gender_output_loss: 0.1587 - image_quality_output_loss: 0.7160 - age_output_loss: 0.9480 - weight_output_loss: 0.6113 - bag_output_loss: 0.5001 - footwear_output_loss: 0.5039 - pose_output_loss: 0.2102 - emotion_output_loss: 0.6141 - gender_output_acc: 0.9362 - image_quality_output_acc: 0.6753 - age_output_acc: 0.6021 - weight_output_acc: 0.7537 - bag_output_acc: 0.7931 - footwear_output_acc: 0.7865 - pose_output_acc: 0.9209 - emotion_output_acc: 0.7706 - val_loss: 8.1664 - val_gender_output_loss: 0.5716 - val_image_quality_output_loss: 1.0342 - val_age_output_loss: 1.6306 - val_weight_output_loss: 1.1544 - val_bag_output_loss: 0.9565 - val_footwear_output_loss: 0.9375 - val_pose_output_loss: 0.8018 - val_emotion_output_loss: 1.0798 - val_gender_output_acc: 0.7984 - val_image_quality_output_acc: 0.5318 - val_age_output_acc: 0.3357 - val_weight_output_acc: 0.5862 - val_bag_output_acc: 0.6179 - val_footwear_output_acc: 0.6285 - val_pose_output_acc: 0.7681 - val_emotion_output_acc: 0.6452\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 6.84337\n",
            "Epoch 20/50\n",
            " - 91s - loss: 3.8755 - gender_output_loss: 0.1490 - image_quality_output_loss: 0.6442 - age_output_loss: 0.8744 - weight_output_loss: 0.5609 - bag_output_loss: 0.4552 - footwear_output_loss: 0.4565 - pose_output_loss: 0.1847 - emotion_output_loss: 0.5507 - gender_output_acc: 0.9395 - image_quality_output_acc: 0.7136 - age_output_acc: 0.6419 - weight_output_acc: 0.7739 - bag_output_acc: 0.8206 - footwear_output_acc: 0.8049 - pose_output_acc: 0.9313 - emotion_output_acc: 0.7909 - val_loss: 8.7208 - val_gender_output_loss: 0.5779 - val_image_quality_output_loss: 1.1332 - val_age_output_loss: 1.7437 - val_weight_output_loss: 1.2703 - val_bag_output_loss: 0.9815 - val_footwear_output_loss: 0.9713 - val_pose_output_loss: 0.8475 - val_emotion_output_loss: 1.1954 - val_gender_output_acc: 0.7928 - val_image_quality_output_acc: 0.5005 - val_age_output_acc: 0.3538 - val_weight_output_acc: 0.5907 - val_bag_output_acc: 0.6225 - val_footwear_output_acc: 0.6346 - val_pose_output_acc: 0.7591 - val_emotion_output_acc: 0.6633\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 6.84337\n",
            "Epoch 21/50\n",
            " - 90s - loss: 3.4593 - gender_output_loss: 0.1304 - image_quality_output_loss: 0.5891 - age_output_loss: 0.7752 - weight_output_loss: 0.4908 - bag_output_loss: 0.3918 - footwear_output_loss: 0.4261 - pose_output_loss: 0.1697 - emotion_output_loss: 0.4861 - gender_output_acc: 0.9466 - image_quality_output_acc: 0.7425 - age_output_acc: 0.6871 - weight_output_acc: 0.8007 - bag_output_acc: 0.8417 - footwear_output_acc: 0.8255 - pose_output_acc: 0.9374 - emotion_output_acc: 0.8179 - val_loss: 9.3348 - val_gender_output_loss: 0.6359 - val_image_quality_output_loss: 1.1722 - val_age_output_loss: 1.8867 - val_weight_output_loss: 1.3249 - val_bag_output_loss: 1.1174 - val_footwear_output_loss: 1.0601 - val_pose_output_loss: 0.8907 - val_emotion_output_loss: 1.2469 - val_gender_output_acc: 0.8115 - val_image_quality_output_acc: 0.5111 - val_age_output_acc: 0.3422 - val_weight_output_acc: 0.5685 - val_bag_output_acc: 0.6174 - val_footwear_output_acc: 0.6421 - val_pose_output_acc: 0.7666 - val_emotion_output_acc: 0.6386\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 6.84337\n",
            "Epoch 22/50\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 6.84337\n",
            "Epoch 21/50\n",
            " - 91s - loss: 3.0203 - gender_output_loss: 0.1193 - image_quality_output_loss: 0.5218 - age_output_loss: 0.6643 - weight_output_loss: 0.4261 - bag_output_loss: 0.3495 - footwear_output_loss: 0.3589 - pose_output_loss: 0.1476 - emotion_output_loss: 0.4328 - gender_output_acc: 0.9533 - image_quality_output_acc: 0.7759 - age_output_acc: 0.7391 - weight_output_acc: 0.8324 - bag_output_acc: 0.8622 - footwear_output_acc: 0.8543 - pose_output_acc: 0.9434 - emotion_output_acc: 0.8396 - val_loss: 9.5275 - val_gender_output_loss: 0.6436 - val_image_quality_output_loss: 1.2030 - val_age_output_loss: 1.9478 - val_weight_output_loss: 1.3536 - val_bag_output_loss: 1.1085 - val_footwear_output_loss: 1.0916 - val_pose_output_loss: 0.9075 - val_emotion_output_loss: 1.2719 - val_gender_output_acc: 0.7918 - val_image_quality_output_acc: 0.5030 - val_age_output_acc: 0.3191 - val_weight_output_acc: 0.5615 - val_bag_output_acc: 0.6149 - val_footwear_output_acc: 0.6240 - val_pose_output_acc: 0.7722 - val_emotion_output_acc: 0.6235\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 6.84337\n",
            "Epoch 23/50\n",
            " - 91s - loss: 2.6933 - gender_output_loss: 0.1110 - image_quality_output_loss: 0.4584 - age_output_loss: 0.5994 - weight_output_loss: 0.3862 - bag_output_loss: 0.3010 - footwear_output_loss: 0.3338 - pose_output_loss: 0.1346 - emotion_output_loss: 0.3688 - gender_output_acc: 0.9591 - image_quality_output_acc: 0.8095 - age_output_acc: 0.7623 - weight_output_acc: 0.8492 - bag_output_acc: 0.8826 - footwear_output_acc: 0.8632 - pose_output_acc: 0.9495 - emotion_output_acc: 0.8637 - val_loss: 10.6521 - val_gender_output_loss: 0.7101 - val_image_quality_output_loss: 1.3530 - val_age_output_loss: 2.2130 - val_weight_output_loss: 1.4994 - val_bag_output_loss: 1.2627 - val_footwear_output_loss: 1.1808 - val_pose_output_loss: 0.9930 - val_emotion_output_loss: 1.4401 - val_gender_output_acc: 0.8080 - val_image_quality_output_acc: 0.4793 - val_age_output_acc: 0.3357 - val_weight_output_acc: 0.5801 - val_bag_output_acc: 0.6114 - val_footwear_output_acc: 0.6210 - val_pose_output_acc: 0.7596 - val_emotion_output_acc: 0.6452\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 6.84337\n",
            "Epoch 24/50\n",
            " - 91s - loss: 2.3179 - gender_output_loss: 0.1034 - image_quality_output_loss: 0.4124 - age_output_loss: 0.4988 - weight_output_loss: 0.3232 - bag_output_loss: 0.2669 - footwear_output_loss: 0.2818 - pose_output_loss: 0.1189 - emotion_output_loss: 0.3124 - gender_output_acc: 0.9615 - image_quality_output_acc: 0.8336 - age_output_acc: 0.8072 - weight_output_acc: 0.8740 - bag_output_acc: 0.8976 - footwear_output_acc: 0.8891 - pose_output_acc: 0.9573 - emotion_output_acc: 0.8889 - val_loss: 12.4366 - val_gender_output_loss: 0.8315 - val_image_quality_output_loss: 1.5209 - val_age_output_loss: 2.4373 - val_weight_output_loss: 1.7746 - val_bag_output_loss: 1.4793 - val_footwear_output_loss: 1.4439 - val_pose_output_loss: 1.1960 - val_emotion_output_loss: 1.7531 - val_gender_output_acc: 0.8095 - val_image_quality_output_acc: 0.4864 - val_age_output_acc: 0.3286 - val_weight_output_acc: 0.5474 - val_bag_output_acc: 0.6048 - val_footwear_output_acc: 0.6255 - val_pose_output_acc: 0.7601 - val_emotion_output_acc: 0.6447\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 6.84337\n",
            "Epoch 25/50\n",
            " - 91s - loss: 2.3179 - gender_output_loss: 0.1034 - image_quality_output_loss: 0.4124 - age_output_loss: 0.4988 - weight_output_loss: 0.3232 - bag_output_loss: 0.2669 - footwear_output_loss: 0.2818 - pose_output_loss: 0.1189 - emotion_output_loss: 0.3124 - gender_output_acc: 0.9615 - image_quality_output_acc: 0.8336 - age_output_acc: 0.8072 - weight_output_acc: 0.8740 - bag_output_acc: 0.8976 - footwear_output_acc: 0.8891 - pose_output_acc: 0.9573 - emotion_output_acc: 0.8889 - val_loss: 12.4366 - val_gender_output_loss: 0.8315 - val_image_quality_output_loss: 1.5209 - val_age_output_loss: 2.4373 - val_weight_output_loss: 1.7746 - val_bag_output_loss: 1.4793 - val_footwear_output_loss: 1.4439 - val_pose_output_loss: 1.1960 - val_emotion_output_loss: 1.7531 - val_gender_output_acc: 0.8095 - val_image_quality_output_acc: 0.4864 - val_age_output_acc: 0.3286 - val_weight_output_acc: 0.5474 - val_bag_output_acc: 0.6048 - val_footwear_output_acc: 0.6255 - val_pose_output_acc: 0.7601 - val_emotion_output_acc: 0.6447\n",
            " - 91s - loss: 2.0177 - gender_output_loss: 0.0938 - image_quality_output_loss: 0.3411 - age_output_loss: 0.4321 - weight_output_loss: 0.2814 - bag_output_loss: 0.2262 - footwear_output_loss: 0.2521 - pose_output_loss: 0.1234 - emotion_output_loss: 0.2676 - gender_output_acc: 0.9661 - image_quality_output_acc: 0.8623 - age_output_acc: 0.8334 - weight_output_acc: 0.8921 - bag_output_acc: 0.9129 - footwear_output_acc: 0.9028 - pose_output_acc: 0.9571 - emotion_output_acc: 0.9040 - val_loss: 12.5850 - val_gender_output_loss: 0.7986 - val_image_quality_output_loss: 1.6141 - val_age_output_loss: 2.5519 - val_weight_output_loss: 1.8371 - val_bag_output_loss: 1.4714 - val_footwear_output_loss: 1.4031 - val_pose_output_loss: 1.1426 - val_emotion_output_loss: 1.7660 - val_gender_output_acc: 0.8100 - val_image_quality_output_acc: 0.5015 - val_age_output_acc: 0.3165 - val_weight_output_acc: 0.5509 - val_bag_output_acc: 0.5993 - val_footwear_output_acc: 0.6174 - val_pose_output_acc: 0.7550 - val_emotion_output_acc: 0.6245\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 6.84337\n",
            "Epoch 26/50\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 6.84337\n",
            "Epoch 26/50\n",
            " - 91s - loss: 1.7904 - gender_output_loss: 0.0889 - image_quality_output_loss: 0.3036 - age_output_loss: 0.3856 - weight_output_loss: 0.2518 - bag_output_loss: 0.2046 - footwear_output_loss: 0.2227 - pose_output_loss: 0.0997 - emotion_output_loss: 0.2335 - gender_output_acc: 0.9669 - image_quality_output_acc: 0.8797 - age_output_acc: 0.8561 - weight_output_acc: 0.9046 - bag_output_acc: 0.9249 - footwear_output_acc: 0.9109 - pose_output_acc: 0.9648 - emotion_output_acc: 0.9115 - val_loss: 12.9918 - val_gender_output_loss: 0.7971 - val_image_quality_output_loss: 1.6473 - val_age_output_loss: 2.6728 - val_weight_output_loss: 1.9275 - val_bag_output_loss: 1.5134 - val_footwear_output_loss: 1.5196 - val_pose_output_loss: 1.1526 - val_emotion_output_loss: 1.7616 - val_gender_output_acc: 0.8085 - val_image_quality_output_acc: 0.4677 - val_age_output_acc: 0.3291 - val_weight_output_acc: 0.5796 - val_bag_output_acc: 0.5832 - val_footwear_output_acc: 0.6321 - val_pose_output_acc: 0.7712 - val_emotion_output_acc: 0.5927\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 6.84337\n",
            "Epoch 27/50\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 6.84337\n",
            "Epoch 27/50\n",
            " - 91s - loss: 1.5911 - gender_output_loss: 0.0720 - image_quality_output_loss: 0.2726 - age_output_loss: 0.3350 - weight_output_loss: 0.2313 - bag_output_loss: 0.1796 - footwear_output_loss: 0.1926 - pose_output_loss: 0.1013 - emotion_output_loss: 0.2065 - gender_output_acc: 0.9739 - image_quality_output_acc: 0.8947 - age_output_acc: 0.8757 - weight_output_acc: 0.9143 - bag_output_acc: 0.9319 - footwear_output_acc: 0.9250 - pose_output_acc: 0.9645 - emotion_output_acc: 0.9263 - val_loss: 12.4830 - val_gender_output_loss: 0.7692 - val_image_quality_output_loss: 1.6580 - val_age_output_loss: 2.4896 - val_weight_output_loss: 1.8269 - val_bag_output_loss: 1.4949 - val_footwear_output_loss: 1.4557 - val_pose_output_loss: 1.0668 - val_emotion_output_loss: 1.7218 - val_gender_output_acc: 0.7913 - val_image_quality_output_acc: 0.4763 - val_age_output_acc: 0.3296 - val_weight_output_acc: 0.5786 - val_bag_output_acc: 0.6038 - val_footwear_output_acc: 0.6285 - val_pose_output_acc: 0.7626 - val_emotion_output_acc: 0.6114\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 6.84337\n",
            "Epoch 28/50\n",
            " - 91s - loss: 1.3717 - gender_output_loss: 0.0679 - image_quality_output_loss: 0.2322 - age_output_loss: 0.2779 - weight_output_loss: 0.1895 - bag_output_loss: 0.1587 - footwear_output_loss: 0.1752 - pose_output_loss: 0.0888 - emotion_output_loss: 0.1816 - gender_output_acc: 0.9757 - image_quality_output_acc: 0.9087 - age_output_acc: 0.8977 - weight_output_acc: 0.9314 - bag_output_acc: 0.9412 - footwear_output_acc: 0.9335 - pose_output_acc: 0.9662 - emotion_output_acc: 0.9348 - val_loss: 12.8543 - val_gender_output_loss: 0.8258 - val_image_quality_output_loss: 1.6801 - val_age_output_loss: 2.5761 - val_weight_output_loss: 1.8427 - val_bag_output_loss: 1.5265 - val_footwear_output_loss: 1.4696 - val_pose_output_loss: 1.0977 - val_emotion_output_loss: 1.8357 - val_gender_output_acc: 0.8065 - val_image_quality_output_acc: 0.4844 - val_age_output_acc: 0.2964 - val_weight_output_acc: 0.5575 - val_bag_output_acc: 0.5907 - val_footwear_output_acc: 0.6300 - val_pose_output_acc: 0.7676 - val_emotion_output_acc: 0.6240\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 6.84337\n",
            "Epoch 29/50\n",
            " - 91s - loss: 1.2209 - gender_output_loss: 0.0661 - image_quality_output_loss: 0.2116 - age_output_loss: 0.2496 - weight_output_loss: 0.1679 - bag_output_loss: 0.1435 - footwear_output_loss: 0.1472 - pose_output_loss: 0.0764 - emotion_output_loss: 0.1585 - gender_output_acc: 0.9744 - image_quality_output_acc: 0.9202 - age_output_acc: 0.9112 - weight_output_acc: 0.9378 - bag_output_acc: 0.9482 - footwear_output_acc: 0.9462 - pose_output_acc: 0.9738 - emotion_output_acc: 0.9415 - val_loss: 13.7985 - val_gender_output_loss: 0.8221 - val_image_quality_output_loss: 1.7999 - val_age_output_loss: 2.8852 - val_weight_output_loss: 2.0150 - val_bag_output_loss: 1.7159 - val_footwear_output_loss: 1.5206 - val_pose_output_loss: 1.1047 - val_emotion_output_loss: 1.9351 - val_gender_output_acc: 0.8070 - val_image_quality_output_acc: 0.4662 - val_age_output_acc: 0.3196 - val_weight_output_acc: 0.5721 - val_bag_output_acc: 0.6124 - val_footwear_output_acc: 0.6245 - val_pose_output_acc: 0.7722 - val_emotion_output_acc: 0.6240\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 6.84337\n",
            "Epoch 30/50\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 6.84337\n",
            "Epoch 30/50\n",
            " - 91s - loss: 1.0984 - gender_output_loss: 0.0641 - image_quality_output_loss: 0.1891 - age_output_loss: 0.2331 - weight_output_loss: 0.1475 - bag_output_loss: 0.1177 - footwear_output_loss: 0.1265 - pose_output_loss: 0.0762 - emotion_output_loss: 0.1441 - gender_output_acc: 0.9770 - image_quality_output_acc: 0.9302 - age_output_acc: 0.9161 - weight_output_acc: 0.9475 - bag_output_acc: 0.9570 - footwear_output_acc: 0.9522 - pose_output_acc: 0.9742 - emotion_output_acc: 0.9479 - val_loss: 13.3806 - val_gender_output_loss: 0.8049 - val_image_quality_output_loss: 1.7307 - val_age_output_loss: 2.7433 - val_weight_output_loss: 1.9664 - val_bag_output_loss: 1.5783 - val_footwear_output_loss: 1.5610 - val_pose_output_loss: 1.1482 - val_emotion_output_loss: 1.8479 - val_gender_output_acc: 0.8110 - val_image_quality_output_acc: 0.4945 - val_age_output_acc: 0.3206 - val_weight_output_acc: 0.5751 - val_bag_output_acc: 0.5917 - val_footwear_output_acc: 0.6200 - val_pose_output_acc: 0.7636 - val_emotion_output_acc: 0.6215\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 6.84337\n",
            "Epoch 31/50\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 6.84337\n",
            "Epoch 31/50\n",
            " - 91s - loss: 0.9821 - gender_output_loss: 0.0542 - image_quality_output_loss: 0.1626 - age_output_loss: 0.1867 - weight_output_loss: 0.1350 - bag_output_loss: 0.1234 - footwear_output_loss: 0.1259 - pose_output_loss: 0.0620 - emotion_output_loss: 0.1323 - gender_output_acc: 0.9810 - image_quality_output_acc: 0.9409 - age_output_acc: 0.9333 - weight_output_acc: 0.9508 - bag_output_acc: 0.9558 - footwear_output_acc: 0.9544 - pose_output_acc: 0.9773 - emotion_output_acc: 0.9562 - val_loss: 14.3846 - val_gender_output_loss: 0.8213 - val_image_quality_output_loss: 1.9354 - val_age_output_loss: 3.0106 - val_weight_output_loss: 2.0681 - val_bag_output_loss: 1.6806 - val_footwear_output_loss: 1.6167 - val_pose_output_loss: 1.1859 - val_emotion_output_loss: 2.0660 - val_gender_output_acc: 0.8059 - val_image_quality_output_acc: 0.4945 - val_age_output_acc: 0.3402 - val_weight_output_acc: 0.5665 - val_bag_output_acc: 0.6159 - val_footwear_output_acc: 0.6154 - val_pose_output_acc: 0.7676 - val_emotion_output_acc: 0.6371\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 6.84337\n",
            "Epoch 32/50\n",
            "\n",
            "Epoch 32/50\n",
            " - 91s - loss: 0.8672 - gender_output_loss: 0.0497 - image_quality_output_loss: 0.1463 - age_output_loss: 0.1724 - weight_output_loss: 0.1184 - bag_output_loss: 0.1006 - footwear_output_loss: 0.1106 - pose_output_loss: 0.0558 - emotion_output_loss: 0.1134 - gender_output_acc: 0.9827 - image_quality_output_acc: 0.9462 - age_output_acc: 0.9402 - weight_output_acc: 0.9564 - bag_output_acc: 0.9641 - footwear_output_acc: 0.9608 - pose_output_acc: 0.9811 - emotion_output_acc: 0.9601 - val_loss: 13.6003 - val_gender_output_loss: 0.7755 - val_image_quality_output_loss: 1.7958 - val_age_output_loss: 2.7863 - val_weight_output_loss: 1.9525 - val_bag_output_loss: 1.6157 - val_footwear_output_loss: 1.5297 - val_pose_output_loss: 1.1024 - val_emotion_output_loss: 2.0424 - val_gender_output_acc: 0.8009 - val_image_quality_output_acc: 0.4693 - val_age_output_acc: 0.3306 - val_weight_output_acc: 0.5554 - val_bag_output_acc: 0.6089 - val_footwear_output_acc: 0.6391 - val_pose_output_acc: 0.7777 - val_emotion_output_acc: 0.6305\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 6.84337\n",
            "Epoch 33/50\n",
            " - 91s - loss: 0.8097 - gender_output_loss: 0.0471 - image_quality_output_loss: 0.1249 - age_output_loss: 0.1694 - weight_output_loss: 0.1111 - bag_output_loss: 0.0904 - footwear_output_loss: 0.1038 - pose_output_loss: 0.0519 - emotion_output_loss: 0.1110 - gender_output_acc: 0.9835 - image_quality_output_acc: 0.9560 - age_output_acc: 0.9416 - weight_output_acc: 0.9609 - bag_output_acc: 0.9688 - footwear_output_acc: 0.9621 - pose_output_acc: 0.9822 - emotion_output_acc: 0.9640 - val_loss: 15.6049 - val_gender_output_loss: 0.8579 - val_image_quality_output_loss: 2.0950 - val_age_output_loss: 3.1773 - val_weight_output_loss: 2.2561 - val_bag_output_loss: 1.8484 - val_footwear_output_loss: 1.7628 - val_pose_output_loss: 1.3128 - val_emotion_output_loss: 2.2947 - val_gender_output_acc: 0.8095 - val_image_quality_output_acc: 0.5076 - val_age_output_acc: 0.3226 - val_weight_output_acc: 0.5978 - val_bag_output_acc: 0.6079 - val_footwear_output_acc: 0.6159 - val_pose_output_acc: 0.7676 - val_emotion_output_acc: 0.6517\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 6.84337\n",
            "Epoch 34/50\n",
            " - 91s - loss: 0.7369 - gender_output_loss: 0.0392 - image_quality_output_loss: 0.1224 - age_output_loss: 0.1498 - weight_output_loss: 0.1039 - bag_output_loss: 0.0887 - footwear_output_loss: 0.0895 - pose_output_loss: 0.0491 - emotion_output_loss: 0.0943 - gender_output_acc: 0.9853 - image_quality_output_acc: 0.9556 - age_output_acc: 0.9482 - weight_output_acc: 0.9640 - bag_output_acc: 0.9694 - footwear_output_acc: 0.9674 - pose_output_acc: 0.9832 - emotion_output_acc: 0.9666 - val_loss: 15.3641 - val_gender_output_loss: 0.9513 - val_image_quality_output_loss: 2.1407 - val_age_output_loss: 3.2008 - val_weight_output_loss: 2.2513 - val_bag_output_loss: 1.7478 - val_footwear_output_loss: 1.7475 - val_pose_output_loss: 1.2342 - val_emotion_output_loss: 2.0905 - val_gender_output_acc: 0.7873 - val_image_quality_output_acc: 0.5091 - val_age_output_acc: 0.3241 - val_weight_output_acc: 0.5907 - val_bag_output_acc: 0.5832 - val_footwear_output_acc: 0.6084 - val_pose_output_acc: 0.7666 - val_emotion_output_acc: 0.5993\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 6.84337\n",
            "Epoch 35/50\n",
            " - 91s - loss: 0.6766 - gender_output_loss: 0.0418 - image_quality_output_loss: 0.1125 - age_output_loss: 0.1331 - weight_output_loss: 0.1011 - bag_output_loss: 0.0784 - footwear_output_loss: 0.0821 - pose_output_loss: 0.0394 - emotion_output_loss: 0.0882 - gender_output_acc: 0.9843 - image_quality_output_acc: 0.9600 - age_output_acc: 0.9530 - weight_output_acc: 0.9668 - bag_output_acc: 0.9722 - footwear_output_acc: 0.9701 - pose_output_acc: 0.9869 - emotion_output_acc: 0.9681 - val_loss: 16.1669 - val_gender_output_loss: 0.9939 - val_image_quality_output_loss: 2.1108 - val_age_output_loss: 3.3733 - val_weight_output_loss: 2.4521 - val_bag_output_loss: 1.9161 - val_footwear_output_loss: 1.7891 - val_pose_output_loss: 1.3037 - val_emotion_output_loss: 2.2279 - val_gender_output_acc: 0.7828 - val_image_quality_output_acc: 0.4803 - val_age_output_acc: 0.3180 - val_weight_output_acc: 0.5312 - val_bag_output_acc: 0.6023 - val_footwear_output_acc: 0.6104 - val_pose_output_acc: 0.7737 - val_emotion_output_acc: 0.6290\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 6.84337\n",
            "Epoch 36/50\n",
            " - 91s - loss: 0.6450 - gender_output_loss: 0.0403 - image_quality_output_loss: 0.1040 - age_output_loss: 0.1298 - weight_output_loss: 0.0925 - bag_output_loss: 0.0792 - footwear_output_loss: 0.0791 - pose_output_loss: 0.0468 - emotion_output_loss: 0.0732 - gender_output_acc: 0.9867 - image_quality_output_acc: 0.9637 - age_output_acc: 0.9571 - weight_output_acc: 0.9686 - bag_output_acc: 0.9720 - footwear_output_acc: 0.9715 - pose_output_acc: 0.9839 - emotion_output_acc: 0.9753 - val_loss: 15.8476 - val_gender_output_loss: 0.8685 - val_image_quality_output_loss: 2.1845 - val_age_output_loss: 3.1443 - val_weight_output_loss: 2.2807 - val_bag_output_loss: 1.8245 - val_footwear_output_loss: 1.8094 - val_pose_output_loss: 1.2710 - val_emotion_output_loss: 2.4647 - val_gender_output_acc: 0.8075 - val_image_quality_output_acc: 0.4965 - val_age_output_acc: 0.3241 - val_weight_output_acc: 0.5675 - val_bag_output_acc: 0.6038 - val_footwear_output_acc: 0.6099 - val_pose_output_acc: 0.7540 - val_emotion_output_acc: 0.6462\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 6.84337\n",
            "Epoch 36/50\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 6.84337\n",
            "Epoch 37/50\n",
            " - 91s - loss: 0.5819 - gender_output_loss: 0.0352 - image_quality_output_loss: 0.0936 - age_output_loss: 0.1117 - weight_output_loss: 0.0819 - bag_output_loss: 0.0671 - footwear_output_loss: 0.0763 - pose_output_loss: 0.0391 - emotion_output_loss: 0.0770 - gender_output_acc: 0.9874 - image_quality_output_acc: 0.9671 - age_output_acc: 0.9606 - weight_output_acc: 0.9700 - bag_output_acc: 0.9754 - footwear_output_acc: 0.9750 - pose_output_acc: 0.9855 - emotion_output_acc: 0.9735 - val_loss: 15.8820 - val_gender_output_loss: 0.8991 - val_image_quality_output_loss: 2.2429 - val_age_output_loss: 3.2141 - val_weight_output_loss: 2.3322 - val_bag_output_loss: 1.8727 - val_footwear_output_loss: 1.7620 - val_pose_output_loss: 1.2307 - val_emotion_output_loss: 2.3283 - val_gender_output_acc: 0.8034 - val_image_quality_output_acc: 0.5121 - val_age_output_acc: 0.3367 - val_weight_output_acc: 0.5847 - val_bag_output_acc: 0.6119 - val_footwear_output_acc: 0.6124 - val_pose_output_acc: 0.7581 - val_emotion_output_acc: 0.6356\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 6.84337\n",
            "Epoch 37/50\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 6.84337\n",
            "Epoch 38/50\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 6.84337\n",
            "Epoch 38/50\n",
            " - 91s - loss: 0.5520 - gender_output_loss: 0.0364 - image_quality_output_loss: 0.0858 - age_output_loss: 0.1069 - weight_output_loss: 0.0814 - bag_output_loss: 0.0633 - footwear_output_loss: 0.0690 - pose_output_loss: 0.0393 - emotion_output_loss: 0.0699 - gender_output_acc: 0.9873 - image_quality_output_acc: 0.9702 - age_output_acc: 0.9628 - weight_output_acc: 0.9715 - bag_output_acc: 0.9774 - footwear_output_acc: 0.9760 - pose_output_acc: 0.9874 - emotion_output_acc: 0.9776 - val_loss: 17.5793 - val_gender_output_loss: 0.9931 - val_image_quality_output_loss: 2.5623 - val_age_output_loss: 3.6215 - val_weight_output_loss: 2.4960 - val_bag_output_loss: 2.1143 - val_footwear_output_loss: 2.0121 - val_pose_output_loss: 1.3865 - val_emotion_output_loss: 2.3934 - val_gender_output_acc: 0.8049 - val_image_quality_output_acc: 0.4824 - val_age_output_acc: 0.3317 - val_weight_output_acc: 0.5751 - val_bag_output_acc: 0.5963 - val_footwear_output_acc: 0.6452 - val_pose_output_acc: 0.7656 - val_emotion_output_acc: 0.6008\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 6.84337\n",
            "Epoch 39/50\n",
            " - 91s - loss: 0.5115 - gender_output_loss: 0.0287 - image_quality_output_loss: 0.0753 - age_output_loss: 0.1011 - weight_output_loss: 0.0694 - bag_output_loss: 0.0697 - footwear_output_loss: 0.0662 - pose_output_loss: 0.0353 - emotion_output_loss: 0.0660 - gender_output_acc: 0.9901 - image_quality_output_acc: 0.9738 - age_output_acc: 0.9662 - weight_output_acc: 0.9760 - bag_output_acc: 0.9766 - footwear_output_acc: 0.9763 - pose_output_acc: 0.9879 - emotion_output_acc: 0.9767 - val_loss: 16.7078 - val_gender_output_loss: 0.9762 - val_image_quality_output_loss: 2.3454 - val_age_output_loss: 3.4106 - val_weight_output_loss: 2.4194 - val_bag_output_loss: 1.9131 - val_footwear_output_loss: 1.8465 - val_pose_output_loss: 1.3262 - val_emotion_output_loss: 2.4704 - val_gender_output_acc: 0.8085 - val_image_quality_output_acc: 0.5045 - val_age_output_acc: 0.3206 - val_weight_output_acc: 0.5736 - val_bag_output_acc: 0.6184 - val_footwear_output_acc: 0.6295 - val_pose_output_acc: 0.7707 - val_emotion_output_acc: 0.6336\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 6.84337\n",
            "Epoch 40/50\n",
            "\n",
            " - 91s - loss: 0.4722 - gender_output_loss: 0.0298 - image_quality_output_loss: 0.0732 - age_output_loss: 0.0887 - weight_output_loss: 0.0641 - bag_output_loss: 0.0572 - footwear_output_loss: 0.0606 - pose_output_loss: 0.0367 - emotion_output_loss: 0.0619 - gender_output_acc: 0.9901 - image_quality_output_acc: 0.9748 - age_output_acc: 0.9699 - weight_output_acc: 0.9779 - bag_output_acc: 0.9813 - footwear_output_acc: 0.9780 - pose_output_acc: 0.9863 - emotion_output_acc: 0.9799 - val_loss: 18.3222 - val_gender_output_loss: 1.0293 - val_image_quality_output_loss: 2.5128 - val_age_output_loss: 3.7699 - val_weight_output_loss: 2.7591 - val_bag_output_loss: 2.0924 - val_footwear_output_loss: 2.0731 - val_pose_output_loss: 1.3772 - val_emotion_output_loss: 2.7084 - val_gender_output_acc: 0.8070 - val_image_quality_output_acc: 0.4879 - val_age_output_acc: 0.3443 - val_weight_output_acc: 0.5978 - val_bag_output_acc: 0.6169 - val_footwear_output_acc: 0.6210 - val_pose_output_acc: 0.7702 - val_emotion_output_acc: 0.6285\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 6.84337\n",
            "Epoch 41/50\n",
            " - 91s - loss: 0.4562 - gender_output_loss: 0.0253 - image_quality_output_loss: 0.0701 - age_output_loss: 0.0880 - weight_output_loss: 0.0653 - bag_output_loss: 0.0523 - footwear_output_loss: 0.0637 - pose_output_loss: 0.0335 - emotion_output_loss: 0.0581 - gender_output_acc: 0.9905 - image_quality_output_acc: 0.9760 - age_output_acc: 0.9696 - weight_output_acc: 0.9768 - bag_output_acc: 0.9821 - footwear_output_acc: 0.9773 - pose_output_acc: 0.9892 - emotion_output_acc: 0.9795 - val_loss: 17.3825 - val_gender_output_loss: 0.9513 - val_image_quality_output_loss: 2.4950 - val_age_output_loss: 3.5839 - val_weight_output_loss: 2.4471 - val_bag_output_loss: 2.0646 - val_footwear_output_loss: 2.0077 - val_pose_output_loss: 1.3293 - val_emotion_output_loss: 2.5035 - val_gender_output_acc: 0.7969 - val_image_quality_output_acc: 0.4919 - val_age_output_acc: 0.3261 - val_weight_output_acc: 0.5817 - val_bag_output_acc: 0.6159 - val_footwear_output_acc: 0.6169 - val_pose_output_acc: 0.7540 - val_emotion_output_acc: 0.6280\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 6.84337\n",
            "Epoch 42/50\n",
            " - 91s - loss: 0.4415 - gender_output_loss: 0.0281 - image_quality_output_loss: 0.0726 - age_output_loss: 0.0813 - weight_output_loss: 0.0657 - bag_output_loss: 0.0597 - footwear_output_loss: 0.0483 - pose_output_loss: 0.0339 - emotion_output_loss: 0.0519 - gender_output_acc: 0.9891 - image_quality_output_acc: 0.9757 - age_output_acc: 0.9727 - weight_output_acc: 0.9779 - bag_output_acc: 0.9782 - footwear_output_acc: 0.9823 - pose_output_acc: 0.9885 - emotion_output_acc: 0.9819 - val_loss: 18.6222 - val_gender_output_loss: 1.0827 - val_image_quality_output_loss: 2.5854 - val_age_output_loss: 3.9414 - val_weight_output_loss: 2.6531 - val_bag_output_loss: 2.0636 - val_footwear_output_loss: 2.0192 - val_pose_output_loss: 1.4484 - val_emotion_output_loss: 2.8283 - val_gender_output_acc: 0.8044 - val_image_quality_output_acc: 0.5040 - val_age_output_acc: 0.3009 - val_weight_output_acc: 0.5731 - val_bag_output_acc: 0.6310 - val_footwear_output_acc: 0.6200 - val_pose_output_acc: 0.7707 - val_emotion_output_acc: 0.6532\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 6.84337\n",
            "Epoch 43/50\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 6.84337\n",
            "Epoch 43/50\n",
            " - 91s - loss: 0.4130 - gender_output_loss: 0.0252 - image_quality_output_loss: 0.0674 - age_output_loss: 0.0783 - weight_output_loss: 0.0569 - bag_output_loss: 0.0485 - footwear_output_loss: 0.0539 - pose_output_loss: 0.0362 - emotion_output_loss: 0.0466 - gender_output_acc: 0.9916 - image_quality_output_acc: 0.9757 - age_output_acc: 0.9743 - weight_output_acc: 0.9788 - bag_output_acc: 0.9829 - footwear_output_acc: 0.9808 - pose_output_acc: 0.9878 - emotion_output_acc: 0.9844 - val_loss: 17.7255 - val_gender_output_loss: 1.0491 - val_image_quality_output_loss: 2.4700 - val_age_output_loss: 3.6764 - val_weight_output_loss: 2.6755 - val_bag_output_loss: 2.0096 - val_footwear_output_loss: 1.9922 - val_pose_output_loss: 1.3436 - val_emotion_output_loss: 2.5091 - val_gender_output_acc: 0.7964 - val_image_quality_output_acc: 0.4884 - val_age_output_acc: 0.3291 - val_weight_output_acc: 0.5640 - val_bag_output_acc: 0.6109 - val_footwear_output_acc: 0.6195 - val_pose_output_acc: 0.7571 - val_emotion_output_acc: 0.6421\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 6.84337\n",
            "Epoch 44/50\n",
            " - 90s - loss: 0.3783 - gender_output_loss: 0.0207 - image_quality_output_loss: 0.0605 - age_output_loss: 0.0743 - weight_output_loss: 0.0525 - bag_output_loss: 0.0451 - footwear_output_loss: 0.0500 - pose_output_loss: 0.0266 - emotion_output_loss: 0.0486 - gender_output_acc: 0.9924 - image_quality_output_acc: 0.9784 - age_output_acc: 0.9756 - weight_output_acc: 0.9820 - bag_output_acc: 0.9832 - footwear_output_acc: 0.9824 - pose_output_acc: 0.9907 - emotion_output_acc: 0.9840 - val_loss: 18.4305 - val_gender_output_loss: 1.0021 - val_image_quality_output_loss: 2.4588 - val_age_output_loss: 3.8142 - val_weight_output_loss: 2.7672 - val_bag_output_loss: 2.1370 - val_footwear_output_loss: 2.0549 - val_pose_output_loss: 1.4515 - val_emotion_output_loss: 2.7448 - val_gender_output_acc: 0.8090 - val_image_quality_output_acc: 0.4904 - val_age_output_acc: 0.3306 - val_weight_output_acc: 0.5867 - val_bag_output_acc: 0.6114 - val_footwear_output_acc: 0.6230 - val_pose_output_acc: 0.7681 - val_emotion_output_acc: 0.6411\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 6.84337\n",
            "Epoch 45/50\n",
            " - 91s - loss: 0.3579 - gender_output_loss: 0.0202 - image_quality_output_loss: 0.0570 - age_output_loss: 0.0683 - weight_output_loss: 0.0494 - bag_output_loss: 0.0413 - footwear_output_loss: 0.0420 - pose_output_loss: 0.0300 - emotion_output_loss: 0.0497 - gender_output_acc: 0.9926 - image_quality_output_acc: 0.9808 - age_output_acc: 0.9780 - weight_output_acc: 0.9830 - bag_output_acc: 0.9863 - footwear_output_acc: 0.9847 - pose_output_acc: 0.9900 - emotion_output_acc: 0.9833 - val_loss: 17.3586 - val_gender_output_loss: 1.0266 - val_image_quality_output_loss: 2.4902 - val_age_output_loss: 3.7234 - val_weight_output_loss: 2.5312 - val_bag_output_loss: 1.9500 - val_footwear_output_loss: 1.9386 - val_pose_output_loss: 1.3188 - val_emotion_output_loss: 2.3798 - val_gender_output_acc: 0.8044 - val_image_quality_output_acc: 0.5000 - val_age_output_acc: 0.3382 - val_weight_output_acc: 0.5726 - val_bag_output_acc: 0.6104 - val_footwear_output_acc: 0.6220 - val_pose_output_acc: 0.7626 - val_emotion_output_acc: 0.6285\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 6.84337\n",
            "Epoch 46/50\n",
            " - 91s - loss: 0.3442 - gender_output_loss: 0.0244 - image_quality_output_loss: 0.0546 - age_output_loss: 0.0668 - weight_output_loss: 0.0436 - bag_output_loss: 0.0369 - footwear_output_loss: 0.0492 - pose_output_loss: 0.0276 - emotion_output_loss: 0.0410 - gender_output_acc: 0.9929 - image_quality_output_acc: 0.9801 - age_output_acc: 0.9787 - weight_output_acc: 0.9840 - bag_output_acc: 0.9869 - footwear_output_acc: 0.9828 - pose_output_acc: 0.9908 - emotion_output_acc: 0.9853 - val_loss: 19.0384 - val_gender_output_loss: 1.1214 - val_image_quality_output_loss: 2.6576 - val_age_output_loss: 3.9769 - val_weight_output_loss: 2.8165 - val_bag_output_loss: 2.2579 - val_footwear_output_loss: 2.0689 - val_pose_output_loss: 1.4084 - val_emotion_output_loss: 2.7306 - val_gender_output_acc: 0.7979 - val_image_quality_output_acc: 0.4824 - val_age_output_acc: 0.3165 - val_weight_output_acc: 0.5549 - val_bag_output_acc: 0.6129 - val_footwear_output_acc: 0.6305 - val_pose_output_acc: 0.7722 - val_emotion_output_acc: 0.6316\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 6.84337\n",
            "Epoch 47/50\n",
            " - 90s - loss: 0.3339 - gender_output_loss: 0.0181 - image_quality_output_loss: 0.0479 - age_output_loss: 0.0639 - weight_output_loss: 0.0484 - bag_output_loss: 0.0413 - footwear_output_loss: 0.0450 - pose_output_loss: 0.0279 - emotion_output_loss: 0.0415 - gender_output_acc: 0.9934 - image_quality_output_acc: 0.9837 - age_output_acc: 0.9777 - weight_output_acc: 0.9840 - bag_output_acc: 0.9840 - footwear_output_acc: 0.9845 - pose_output_acc: 0.9905 - emotion_output_acc: 0.9849 - val_loss: 17.8430 - val_gender_output_loss: 1.0902 - val_image_quality_output_loss: 2.4829 - val_age_output_loss: 3.7392 - val_weight_output_loss: 2.7056 - val_bag_output_loss: 1.9669 - val_footwear_output_loss: 2.0339 - val_pose_output_loss: 1.3391 - val_emotion_output_loss: 2.4851 - val_gender_output_acc: 0.7939 - val_image_quality_output_acc: 0.5015 - val_age_output_acc: 0.3221 - val_weight_output_acc: 0.5731 - val_bag_output_acc: 0.6275 - val_footwear_output_acc: 0.6104 - val_pose_output_acc: 0.7843 - val_emotion_output_acc: 0.6053\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 6.84337\n",
            "Epoch 48/50\n",
            " - 91s - loss: 0.3092 - gender_output_loss: 0.0175 - image_quality_output_loss: 0.0463 - age_output_loss: 0.0523 - weight_output_loss: 0.0464 - bag_output_loss: 0.0380 - footwear_output_loss: 0.0440 - pose_output_loss: 0.0213 - emotion_output_loss: 0.0434 - gender_output_acc: 0.9941 - image_quality_output_acc: 0.9854 - age_output_acc: 0.9831 - weight_output_acc: 0.9848 - bag_output_acc: 0.9860 - footwear_output_acc: 0.9849 - pose_output_acc: 0.9931 - emotion_output_acc: 0.9856 - val_loss: 17.8519 - val_gender_output_loss: 1.0452 - val_image_quality_output_loss: 2.6523 - val_age_output_loss: 3.8380 - val_weight_output_loss: 2.5708 - val_bag_output_loss: 2.0616 - val_footwear_output_loss: 1.9604 - val_pose_output_loss: 1.3126 - val_emotion_output_loss: 2.4109 - val_gender_output_acc: 0.8004 - val_image_quality_output_acc: 0.4970 - val_age_output_acc: 0.3443 - val_weight_output_acc: 0.5670 - val_bag_output_acc: 0.6149 - val_footwear_output_acc: 0.6295 - val_pose_output_acc: 0.7681 - val_emotion_output_acc: 0.6200\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 6.84337\n",
            "Epoch 48/50\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 6.84337\n",
            "Epoch 49/50\n",
            " - 90s - loss: 0.3015 - gender_output_loss: 0.0192 - image_quality_output_loss: 0.0450 - age_output_loss: 0.0609 - weight_output_loss: 0.0409 - bag_output_loss: 0.0392 - footwear_output_loss: 0.0358 - pose_output_loss: 0.0244 - emotion_output_loss: 0.0361 - gender_output_acc: 0.9925 - image_quality_output_acc: 0.9833 - age_output_acc: 0.9793 - weight_output_acc: 0.9859 - bag_output_acc: 0.9860 - footwear_output_acc: 0.9885 - pose_output_acc: 0.9924 - emotion_output_acc: 0.9874 - val_loss: 18.6868 - val_gender_output_loss: 1.0442 - val_image_quality_output_loss: 2.7377 - val_age_output_loss: 3.9068 - val_weight_output_loss: 2.7831 - val_bag_output_loss: 2.1337 - val_footwear_output_loss: 2.0291 - val_pose_output_loss: 1.3359 - val_emotion_output_loss: 2.7163 - val_gender_output_acc: 0.7928 - val_image_quality_output_acc: 0.5040 - val_age_output_acc: 0.3317 - val_weight_output_acc: 0.5640 - val_bag_output_acc: 0.6069 - val_footwear_output_acc: 0.6220 - val_pose_output_acc: 0.7752 - val_emotion_output_acc: 0.6583\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 6.84337\n",
            "Epoch 49/50\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 6.84337\n",
            "Epoch 50/50\n",
            " - 91s - loss: 0.2929 - gender_output_loss: 0.0181 - image_quality_output_loss: 0.0453 - age_output_loss: 0.0558 - weight_output_loss: 0.0409 - bag_output_loss: 0.0386 - footwear_output_loss: 0.0405 - pose_output_loss: 0.0206 - emotion_output_loss: 0.0330 - gender_output_acc: 0.9928 - image_quality_output_acc: 0.9846 - age_output_acc: 0.9812 - weight_output_acc: 0.9870 - bag_output_acc: 0.9865 - footwear_output_acc: 0.9863 - pose_output_acc: 0.9926 - emotion_output_acc: 0.9885 - val_loss: 19.9268 - val_gender_output_loss: 1.1628 - val_image_quality_output_loss: 2.8557 - val_age_output_loss: 4.3239 - val_weight_output_loss: 2.8353 - val_bag_output_loss: 2.2498 - val_footwear_output_loss: 2.2813 - val_pose_output_loss: 1.4869 - val_emotion_output_loss: 2.7312 - val_gender_output_acc: 0.8029 - val_image_quality_output_acc: 0.4869 - val_age_output_acc: 0.3337 - val_weight_output_acc: 0.5570 - val_bag_output_acc: 0.6205 - val_footwear_output_acc: 0.6235 - val_pose_output_acc: 0.7671 - val_emotion_output_acc: 0.6260\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 6.84337\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd9d4789208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}